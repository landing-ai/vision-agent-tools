{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9968fc-11b3-4513-b277-67869a4d99f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import base64\n",
    "import tempfile\n",
    "from enum import Enum\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Dict, Any, Tuple, cast\n",
    "\n",
    "import av\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from vision_agent_tools.models.florence2_sam2 import Florence2SAM2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd926b2-5c8d-4f04-8a07-b118119d90b2",
   "metadata": {},
   "source": [
    "# Visualization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8339e1e0-e5e6-4fa3-8310-ceb3834184dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_bbox(\n",
    "    bbox: List[Union[int, float]], image_size: Tuple[int, ...]\n",
    ") -> List[float]:\n",
    "    r\"\"\"DeNormalize the bounding box coordinates so that they are in absolute values.\"\"\"\n",
    "\n",
    "    if len(bbox) != 4:\n",
    "        raise ValueError(\"Bounding box must be of length 4.\")\n",
    "\n",
    "    arr = np.array(bbox)\n",
    "    if np.all((arr >= 0) & (arr <= 1)):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        x1 = round(x1 * image_size[1])\n",
    "        y1 = round(y1 * image_size[0])\n",
    "        x2 = round(x2 * image_size[1])\n",
    "        y2 = round(y2 * image_size[0])\n",
    "        return [x1, y1, x2, y2]\n",
    "    else:\n",
    "        return bbox\n",
    "\n",
    "COLORS = [\n",
    "    (158, 218, 229),\n",
    "    (219, 219, 141),\n",
    "    (23, 190, 207),\n",
    "    (188, 189, 34),\n",
    "    (199, 199, 199),\n",
    "    (247, 182, 210),\n",
    "    (127, 127, 127),\n",
    "    (227, 119, 194),\n",
    "    (196, 156, 148),\n",
    "    (197, 176, 213),\n",
    "    (140, 86, 75),\n",
    "    (148, 103, 189),\n",
    "    (255, 152, 150),\n",
    "    (152, 223, 138),\n",
    "    (214, 39, 40),\n",
    "    (44, 160, 44),\n",
    "    (255, 187, 120),\n",
    "    (174, 199, 232),\n",
    "    (255, 127, 14),\n",
    "    (31, 119, 180),\n",
    "]\n",
    "\n",
    "def _get_text_coords_from_mask(\n",
    "    mask: np.ndarray, v_gap: int = 10, h_gap: int = 10\n",
    ") -> Tuple[int, int]:\n",
    "    mask = mask.astype(np.uint8)\n",
    "    if np.sum(mask) == 0:\n",
    "        return (0, 0)\n",
    "\n",
    "    rows, cols = np.nonzero(mask)\n",
    "    top = rows.min()\n",
    "    bottom = rows.max()\n",
    "    left = cols.min()\n",
    "    right = cols.max()\n",
    "\n",
    "    if top - v_gap < 0:\n",
    "        if bottom + v_gap > mask.shape[0]:\n",
    "            top = top\n",
    "        else:\n",
    "            top = bottom + v_gap\n",
    "    else:\n",
    "        top = top - v_gap\n",
    "\n",
    "    return left + (right - left) // 2 - h_gap, top\n",
    "\n",
    "def overlay_segmentation_masks(\n",
    "    medias: Union[np.ndarray, List[np.ndarray]],\n",
    "    masks: Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]],\n",
    "    draw_label: bool = True,\n",
    "    secondary_label_key: str = \"tracking_label\",\n",
    ") -> Union[np.ndarray, List[np.ndarray]]:\n",
    "    \"\"\"'overlay_segmentation_masks' is a utility function that displays segmentation\n",
    "    masks.\n",
    "\n",
    "    Parameters:\n",
    "        medias (Union[np.ndarray, List[np.ndarray]]): The image or frames to display\n",
    "            the masks on.\n",
    "        masks (Union[List[Dict[str, Any]], List[List[Dict[str, Any]]]]): A list of\n",
    "            dictionaries or a list of list of dictionaries containing the masks, labels\n",
    "            and scores.\n",
    "        draw_label (bool, optional): If True, the labels will be displayed on the image.\n",
    "        secondary_label_key (str, optional): The key to use for the secondary\n",
    "            tracking label which is needed in videos to display tracking information.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The image with the masks displayed.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "        >>> image_with_masks = overlay_segmentation_masks(\n",
    "            image,\n",
    "            [{\n",
    "                'score': 0.99,\n",
    "                'label': 'dinosaur',\n",
    "                'mask': array([[0, 0, 0, ..., 0, 0, 0],\n",
    "                    [0, 0, 0, ..., 0, 0, 0],\n",
    "                    ...,\n",
    "                    [0, 0, 0, ..., 0, 0, 0],\n",
    "                    [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
    "            }],\n",
    "        )\n",
    "    \"\"\"\n",
    "    medias_int: List[np.ndarray] = (\n",
    "        [medias] if isinstance(medias, np.ndarray) else medias\n",
    "    )\n",
    "    masks_int = [masks] if isinstance(masks[0], dict) else masks\n",
    "    masks_int = cast(List[List[Dict[str, Any]]], masks_int)\n",
    "\n",
    "    labels = set()\n",
    "    for mask_i in masks_int:\n",
    "        for mask_j in mask_i:\n",
    "            labels.add(mask_j[\"label\"])\n",
    "    color = {label: COLORS[i % len(COLORS)] for i, label in enumerate(labels)}\n",
    "\n",
    "    width, height = Image.fromarray(medias_int[0]).size\n",
    "    fontsize = max(12, int(min(width, height) / 40))\n",
    "    font = ImageFont.truetype(\"/home/ec2-user/code/vision-agent-tools/tmp/default_font_ch_en.ttf\",fontsize)\n",
    "\n",
    "    frame_out = []\n",
    "    for i, frame in enumerate(medias_int):\n",
    "        pil_image = Image.fromarray(frame.astype(np.uint8)).convert(\"RGBA\")\n",
    "        for elt in masks_int[i]:\n",
    "            mask = elt[\"mask\"]\n",
    "            label = elt[\"label\"]\n",
    "            tracking_lbl = elt.get(secondary_label_key, None)\n",
    "            np_mask = np.zeros((pil_image.size[1], pil_image.size[0], 4))\n",
    "            np_mask[mask > 0, :] = color[label] + (255 * 0.5,)\n",
    "            mask_img = Image.fromarray(np_mask.astype(np.uint8))\n",
    "            pil_image = Image.alpha_composite(pil_image, mask_img)\n",
    "\n",
    "            if draw_label:\n",
    "                draw = ImageDraw.Draw(pil_image)\n",
    "                text = tracking_lbl if tracking_lbl else label\n",
    "                text_box = draw.textbbox((0, 0), text=text, font=font)\n",
    "                x, y = _get_text_coords_from_mask(\n",
    "                    mask,\n",
    "                    v_gap=(text_box[3] - text_box[1]) + 10,\n",
    "                    h_gap=(text_box[2] - text_box[0]) // 2,\n",
    "                )\n",
    "                if x != 0 and y != 0:\n",
    "                    text_box = draw.textbbox((x, y), text=text, font=font)\n",
    "                    draw.rectangle((x, y, text_box[2], text_box[3]), fill=color[label])\n",
    "                    draw.text((x, y), text, fill=\"black\", font=font)\n",
    "        frame_out.append(np.array(pil_image))\n",
    "    return frame_out[0] if len(frame_out) == 1 else frame_out\n",
    "\n",
    "def load_video(video_path: str):\n",
    "    with open(video_path, \"rb\") as f:\n",
    "        video_bytes = f.read()\n",
    "        with tempfile.NamedTemporaryFile() as fp:\n",
    "            fp.write(video_bytes)\n",
    "            fp.flush()\n",
    "            video_temp_file = fp.name\n",
    "            cap = cv2.VideoCapture(video_temp_file)\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "            frames = []\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "            return np.array(frames), fps\n",
    "\n",
    "def rle_decode_array(rle: dict[str, list[int]]) -> np.ndarray:\n",
    "    r\"\"\"Decode a run-length encoded mask. Returns numpy array, 1 - mask, 0 - background.\n",
    "\n",
    "    Parameters:\n",
    "        rle: The run-length encoded mask.\n",
    "    \"\"\"\n",
    "    size = rle[\"size\"]\n",
    "    counts = rle[\"counts\"]\n",
    "\n",
    "    total_elements = size[0] * size[1]\n",
    "    flattened_mask = np.zeros(total_elements, dtype=np.uint8)\n",
    "\n",
    "    current_pos = 0\n",
    "    for i, count in enumerate(counts):\n",
    "        if i % 2 == 1:\n",
    "            flattened_mask[current_pos : current_pos + count] = 1\n",
    "        current_pos += count\n",
    "\n",
    "    binary_mask = flattened_mask.reshape(size, order=\"F\")\n",
    "    return binary_mask\n",
    "\n",
    "def _resize_frame(frame: np.ndarray) -> np.ndarray:\n",
    "    height, width = frame.shape[:2]\n",
    "    new_width = width - (width % 2)\n",
    "    new_height = height - (height % 2)\n",
    "    return cv2.resize(frame, (new_width, new_height))\n",
    "\n",
    "class FileSerializer:\n",
    "    \"\"\"Adaptor class that allows IPython.display.display() to serialize a file to a\n",
    "    base64 string representation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_uri: str):\n",
    "        self.video_uri = file_uri\n",
    "        assert os.path.isfile(\n",
    "            file_uri\n",
    "        ), f\"Only support local files currently: {file_uri}\"\n",
    "        assert Path(file_uri).exists(), f\"File not found: {file_uri}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"FileSerializer({self.video_uri})\"\n",
    "\n",
    "    def base64(self) -> str:\n",
    "        with open(self.video_uri, \"rb\") as file:\n",
    "            return base64.b64encode(file.read()).decode(\"utf-8\")\n",
    "\n",
    "class MimeType(str, Enum):\n",
    "    \"\"\"Represents a MIME type.\"\"\"\n",
    "\n",
    "    TEXT_PLAIN = \"text/plain\"\n",
    "    TEXT_HTML = \"text/html\"\n",
    "    TEXT_MARKDOWN = \"text/markdown\"\n",
    "    IMAGE_SVG = \"image/svg+xml\"\n",
    "    IMAGE_PNG = \"image/png\"\n",
    "    IMAGE_JPEG = \"image/jpeg\"\n",
    "    VIDEO_MP4_B64 = \"video/mp4/base64\"\n",
    "    APPLICATION_PDF = \"application/pdf\"\n",
    "    TEXT_LATEX = \"text/latex\"\n",
    "    APPLICATION_JSON = \"application/json\"\n",
    "    APPLICATION_JAVASCRIPT = \"application/javascript\"\n",
    "    APPLICATION_ARTIFACT = \"application/artifact\"\n",
    "\n",
    "def video_writer(\n",
    "    frames: List[np.ndarray], fps: float = 1.0, filename: str | None = None\n",
    ") -> str:\n",
    "    if filename is None:\n",
    "        filename = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\").name\n",
    "    container = av.open(filename, mode=\"w\")\n",
    "    stream = container.add_stream(\"h264\", rate=fps)\n",
    "    height, width = frames[0].shape[:2]\n",
    "    stream.height = height - (height % 2)\n",
    "    stream.width = width - (width % 2)\n",
    "    stream.pix_fmt = \"yuv420p\"\n",
    "    stream.options = {\"crf\": \"10\"}\n",
    "    for frame in frames:\n",
    "        # Remove the alpha channel (convert RGBA to RGB)\n",
    "        frame_rgb = frame[:, :, :3]\n",
    "        # Resize the frame to make dimensions divisible by 2\n",
    "        frame_rgb = _resize_frame(frame_rgb)\n",
    "        av_frame = av.VideoFrame.from_ndarray(frame_rgb, format=\"rgb24\")\n",
    "        for packet in stream.encode(av_frame):\n",
    "            container.mux(packet)\n",
    "\n",
    "    for packet in stream.encode():\n",
    "        container.mux(packet)\n",
    "    container.close()\n",
    "    return filename\n",
    "\n",
    "def _save_video_to_result(video_uri: str) -> None:\n",
    "    \"\"\"Saves a video into the result of the code execution (as an intermediate output).\"\"\"\n",
    "    from IPython.display import display\n",
    "\n",
    "    serializer = FileSerializer(video_uri)\n",
    "    display(\n",
    "        {\n",
    "            MimeType.VIDEO_MP4_B64: serializer.base64(),\n",
    "            MimeType.TEXT_PLAIN: str(serializer),\n",
    "        },\n",
    "        raw=True,\n",
    "    )\n",
    "\n",
    "def save_video(\n",
    "    frames: List[np.ndarray], output_video_path: str | None = None, fps: float = 1\n",
    ") -> str:\n",
    "    \"\"\"'save_video' is a utility function that saves a list of frames as a mp4 video file on disk.\n",
    "\n",
    "    Parameters:\n",
    "        frames (list[np.ndarray]): A list of frames to save.\n",
    "        output_video_path (str): The path to save the video file. If not provided, a temporary file will be created.\n",
    "        fps (float): The number of frames composes a second in the video.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the saved video file.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "        >>> save_video(frames)\n",
    "        \"/tmp/tmpvideo123.mp4\"\n",
    "    \"\"\"\n",
    "    if fps <= 0:\n",
    "        raise ValueError(f\"fps must be greater than 0 got {fps}\")\n",
    "\n",
    "    if not isinstance(frames, list) or len(frames) == 0:\n",
    "        raise ValueError(\"Frames must be a list of NumPy arrays\")\n",
    "\n",
    "    for frame in frames:\n",
    "        if not isinstance(frame, np.ndarray) or (\n",
    "            frame.shape[0] == 0 and frame.shape[1] == 0\n",
    "        ):\n",
    "            raise ValueError(\"A frame is not a valid NumPy array with shape (H, W, C)\")\n",
    "\n",
    "    if output_video_path is None:\n",
    "        output_video_path = tempfile.NamedTemporaryFile(\n",
    "            delete=False, suffix=\".mp4\"\n",
    "        ).name\n",
    "\n",
    "    output_video_path = video_writer(frames, fps, output_video_path)\n",
    "    _save_video_to_result(output_video_path)\n",
    "    return output_video_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc9e31-f02f-40b9-ad86-cb628b2b5097",
   "metadata": {},
   "source": [
    "# Run inference on the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f8b038-8d52-4665-821e-5fa6d407d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your video\n",
    "video_path = \"../tests/shared_data/videos/shark_10fps.mp4\"\n",
    "# video_path = \"../tests/shared_data/videos/tracking_car.mp4\"\n",
    "\n",
    "# Load the video\n",
    "frames, fps = load_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a92ae-d17a-46e5-89d9-e4fa8de4ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Florence2SAM2 instance\n",
    "florence2_sam2 = Florence2SAM2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70898011-c1ee-41fa-8444-6d53982066b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "results = florence2_sam2(video=frames, prompt=\"shark, person\")\n",
    "# results = florence2_sam2(video=frames, prompt=\"car\")\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe932d1e-e995-4fbd-8c88-c6075c54cf24",
   "metadata": {},
   "source": [
    "# Visulalize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78134b8b-35fa-4428-b125-16c6a94057ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "return_data = []\n",
    "for frame_idx in range(len(results)):\n",
    "    return_frame_data = []\n",
    "    annotations = len(results[frame_idx])\n",
    "    for annotation_idx in range(annotations):\n",
    "        annotation = results[frame_idx][annotation_idx]\n",
    "        mask = rle_decode_array(annotation[\"mask\"])\n",
    "        label = str(annotation[\"id\"]) + \": \" + annotation[\"label\"]\n",
    "        return_frame_data.append({\"label\": label, \"mask\": mask, \"score\": 1.0})\n",
    "    return_data.append(return_frame_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39148eb3-267a-47b0-ab2c-33e93f12ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_frames = []\n",
    "for frame, frame_detections in zip(frames, return_data):\n",
    "    frame_with_overlays = frame.copy()\n",
    "    frame_with_overlays = overlay_segmentation_masks(frame_with_overlays, frame_detections)\n",
    "    processed_frames.append(frame_with_overlays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bef6e0-ff06-41b4-9e61-5a387d732d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../tmp\", exist_ok=True)\n",
    "filepath = \"../tmp/florence2sam2_shark.mp4\"\n",
    "# filepath = \"../tmp/florence2sam2_track_car.mp4\"\n",
    "output_path = save_video(processed_frames, filepath, fps=int(fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bef6e0-ff06-41b4-9e61-5a387d732d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(\"out_shape\", frame_with_overlays.shape)\n",
    "data = frame_with_overlays\n",
    "plt.imshow(data, interpolation='nearest')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-kernel",
   "language": "python",
   "name": "my-python3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
